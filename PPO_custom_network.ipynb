{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19460f5b-8ba1-46f9-b59f-ce2e8c0df28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edu/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/edu/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import numpy as np \n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8d6d70-4486-466b-b9a1-9c024d60e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "from gym import Wrapper, ObservationWrapper\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
    "\n",
    "class CustomWrapper(Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        kwargs.pop('seed', None)  # Remove the 'seed' argument\n",
    "        kwargs.pop('options', None)  # Remove the 'options' argument\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "# Wrap your environment\n",
    "env = CustomWrapper(env)\n",
    "\n",
    "class SkipFrame(Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "    \n",
    "class RemoveChannelDim(ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return np.squeeze(observation)\n",
    "    \n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = RemoveChannelDim(env)\n",
    "env = FrameStack(env, num_stack=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0e2bf4-3191-4d21-856e-c89d6ec672d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 23:17:09.705229: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-10 23:17:10.756027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f340c2cf-05a7-47ec-9819-e721bda0cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39420877-b03e-4aa1-a9c4-a2df2d9a9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './ppo/train/'\n",
    "LOG_DIR = './ppo/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be196ae0-9bb0-4d1d-94bf-66d8f7c9d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model saving callback\n",
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c76edfda-e91d-4cec-9f16-e81a964e3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_input_channels, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(\n",
    "                th.as_tensor(observation_space.sample()[None]).float()\n",
    "            ).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(3136, 512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(512, features_dim))\n",
    "            #nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb86b026-0801-4a3f-a731-a9c90e040bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edu/anaconda3/envs/tp_ap/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, \n",
    "            n_steps=512, policy_kwargs=policy_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48139e23-5592-4382-a924-24f0e66d714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo/logs/\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 33  |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 15  |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 1024          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0591466e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.00027       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 760           |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -9.38e-05     |\n",
      "|    value_loss           | 2.47e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.26e+03      |\n",
      "|    ep_rew_mean          | 1.7e+03       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 28            |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 54            |\n",
      "|    total_timesteps      | 1536          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0504154e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.000789      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 201           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -6.95e-05     |\n",
      "|    value_loss           | 288           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 854           |\n",
      "|    ep_rew_mean          | 1.6e+03       |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 73            |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5367203e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.000597      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 3.43e+03      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -1.2e-05      |\n",
      "|    value_loss           | 7.06e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 854         |\n",
      "|    ep_rew_mean          | 1.6e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.70671e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.000596    |\n",
      "|    learning_rate        | 1e-06       |\n",
      "|    loss                 | 1.48e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 3.78e-05    |\n",
      "|    value_loss           | 3.29e+03    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 890           |\n",
      "|    ep_rew_mean          | 1.65e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 112           |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7799979e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.00025      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 509           |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.000125     |\n",
      "|    value_loss           | 1e+03         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 876           |\n",
      "|    ep_rew_mean          | 1.63e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 132           |\n",
      "|    total_timesteps      | 3584          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3597614e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -7.15e-07     |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 1.73e+03      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -1.33e-05     |\n",
      "|    value_loss           | 3.17e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 876           |\n",
      "|    ep_rew_mean          | 1.63e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 27            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 150           |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.8440644e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.000282      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 2.06e+03      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -4.69e-05     |\n",
      "|    value_loss           | 3.3e+03       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40000\u001b[39m, callback\u001b[38;5;241m=\u001b[39mcallback)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set new logger \u001b[39;00m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:188\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/shimmy/openai_gym_compatibility.py:123\u001b[0m, in \u001b[0;36mGymV26CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/wrappers/frame_stack.py:173\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36mSkipFrame.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     22\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Accumulate reward and repeat the same action\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     obs, reward, done, trunk, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/core.py:319\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[ObsType, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment with action.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/nes_py/wrappers/joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/wrappers/compatibility.py:105\u001b[0m, in \u001b[0;36mEnvCompatibility.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Dict]:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/envs/tp_ap/lib/python3.9/site-packages/nes_py/nes_env.py:300\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrollers[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# pass the action to the emulator as an unsigned byte\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# get the reward for this step\u001b[39;00m\n\u001b[1;32m    302\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reward())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "def train_model():\n",
    "    env.reset()\n",
    "    new_logger = configure(LOG_DIR, [\"stdout\", \"csv\"])\n",
    "    # Set new logger \n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    model.learn(total_timesteps=40000, callback=callback)\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd75fa-1b53-4b37-a8cb-d83d9dace552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310d790-3739-412f-951f-fe4d1a3a7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de26ad-6af3-4dff-b805-7223ea41142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "#model = PPO.load('test_model')\n",
    "## Start the game \n",
    "#state = env.reset()\n",
    "#print(type(state))\n",
    "#print(state)\n",
    "#print(env.action_space)\n",
    "#observation_shape = state[0].shape\n",
    "#print(observation_shape)\n",
    "## Loop through the game\n",
    "#while True: \n",
    "#    action, _states = model.predict(state[0])\n",
    "#    state, reward, done, info = env.step(action)\n",
    "#\n",
    "#    env.render()\n",
    "#\n",
    "#    if done:\n",
    "#        state = env.reset()\n",
    "#        break  # Break the loop if the game is over\n",
    "\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "model = PPO.load(\"test_model\")\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "#cr = 0\n",
    "#while True:\n",
    "#    action, _states = model.predict(obs, deterministic=False)\n",
    "#    obs, rewards, done, info = env.step(action)\n",
    "#    env.step(action)\n",
    "#    cr += rewards\n",
    "#    print(\"Reward: {}\\t\\t\".format(cr), end='\\r')\n",
    "#    env.render()\n",
    "#    if (done):\n",
    "#        print(\"Finished an episode with total reward: \", cr)\n",
    "#        cr = 0\n",
    "#        break\n",
    "\n",
    "\n",
    "print(evaluate_policy(model, env, n_eval_episodes=2, deterministic=False, render=True))\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3732799-a499-4a62-83cb-d2b5ac43ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52991f29-9fd3-42dc-b9eb-9e5cd59a202c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time/fps</th>\n",
       "      <th>time/total_timesteps</th>\n",
       "      <th>time/iterations</th>\n",
       "      <th>time/time_elapsed</th>\n",
       "      <th>train/value_loss</th>\n",
       "      <th>train/learning_rate</th>\n",
       "      <th>train/clip_fraction</th>\n",
       "      <th>train/n_updates</th>\n",
       "      <th>train/clip_range</th>\n",
       "      <th>train/explained_variance</th>\n",
       "      <th>train/entropy_loss</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>train/policy_gradient_loss</th>\n",
       "      <th>train/approx_kl</th>\n",
       "      <th>rollout/ep_rew_mean</th>\n",
       "      <th>rollout/ep_len_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>1024</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>2473.499306</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.697110e-04</td>\n",
       "      <td>-1.945902</td>\n",
       "      <td>760.137024</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>1.059147e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>1536</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>287.685600</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.889271e-04</td>\n",
       "      <td>-1.945902</td>\n",
       "      <td>200.912506</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>1.050415e-06</td>\n",
       "      <td>1701.000000</td>\n",
       "      <td>1265.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>7064.046777</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.965233e-04</td>\n",
       "      <td>-1.945901</td>\n",
       "      <td>3429.449951</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>6.536720e-07</td>\n",
       "      <td>1600.500000</td>\n",
       "      <td>854.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>2560</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>3292.192752</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.963445e-04</td>\n",
       "      <td>-1.945902</td>\n",
       "      <td>1477.534302</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>9.706710e-07</td>\n",
       "      <td>1600.500000</td>\n",
       "      <td>854.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>3072</td>\n",
       "      <td>6</td>\n",
       "      <td>112</td>\n",
       "      <td>999.817033</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-2.502203e-04</td>\n",
       "      <td>-1.945902</td>\n",
       "      <td>509.015228</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>2.779998e-06</td>\n",
       "      <td>1646.666667</td>\n",
       "      <td>889.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27</td>\n",
       "      <td>3584</td>\n",
       "      <td>7</td>\n",
       "      <td>132</td>\n",
       "      <td>3170.068651</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-7.152557e-07</td>\n",
       "      <td>-1.945899</td>\n",
       "      <td>1728.846558</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>5.359761e-07</td>\n",
       "      <td>1625.500000</td>\n",
       "      <td>876.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>4096</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>3296.370892</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.819300e-04</td>\n",
       "      <td>-1.945894</td>\n",
       "      <td>2055.074951</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>7.844064e-07</td>\n",
       "      <td>1625.500000</td>\n",
       "      <td>876.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time/fps  time/total_timesteps  time/iterations  time/time_elapsed  \\\n",
       "0        33                   512                1                 15   \n",
       "1        28                  1024                2                 35   \n",
       "2        28                  1536                3                 54   \n",
       "3        27                  2048                4                 73   \n",
       "4        27                  2560                5                 92   \n",
       "5        27                  3072                6                112   \n",
       "6        27                  3584                7                132   \n",
       "7        27                  4096                8                150   \n",
       "\n",
       "   train/value_loss  train/learning_rate  train/clip_fraction  \\\n",
       "0               NaN                  NaN                  NaN   \n",
       "1       2473.499306             0.000001                  0.0   \n",
       "2        287.685600             0.000001                  0.0   \n",
       "3       7064.046777             0.000001                  0.0   \n",
       "4       3292.192752             0.000001                  0.0   \n",
       "5        999.817033             0.000001                  0.0   \n",
       "6       3170.068651             0.000001                  0.0   \n",
       "7       3296.370892             0.000001                  0.0   \n",
       "\n",
       "   train/n_updates  train/clip_range  train/explained_variance  \\\n",
       "0              NaN               NaN                       NaN   \n",
       "1             10.0               0.2              2.697110e-04   \n",
       "2             20.0               0.2              7.889271e-04   \n",
       "3             30.0               0.2              5.965233e-04   \n",
       "4             40.0               0.2              5.963445e-04   \n",
       "5             50.0               0.2             -2.502203e-04   \n",
       "6             60.0               0.2             -7.152557e-07   \n",
       "7             70.0               0.2              2.819300e-04   \n",
       "\n",
       "   train/entropy_loss   train/loss  train/policy_gradient_loss  \\\n",
       "0                 NaN          NaN                         NaN   \n",
       "1           -1.945902   760.137024                   -0.000094   \n",
       "2           -1.945902   200.912506                   -0.000070   \n",
       "3           -1.945901  3429.449951                   -0.000012   \n",
       "4           -1.945902  1477.534302                    0.000038   \n",
       "5           -1.945902   509.015228                   -0.000125   \n",
       "6           -1.945899  1728.846558                   -0.000013   \n",
       "7           -1.945894  2055.074951                   -0.000047   \n",
       "\n",
       "   train/approx_kl  rollout/ep_rew_mean  rollout/ep_len_mean  \n",
       "0              NaN                  NaN                  NaN  \n",
       "1     1.059147e-06                  NaN                  NaN  \n",
       "2     1.050415e-06          1701.000000          1265.000000  \n",
       "3     6.536720e-07          1600.500000           854.500000  \n",
       "4     9.706710e-07          1600.500000           854.500000  \n",
       "5     2.779998e-06          1646.666667           889.666667  \n",
       "6     5.359761e-07          1625.500000           876.000000  \n",
       "7     7.844064e-07          1625.500000           876.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ppo/logs/progress.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d5b1e8-48f5-4294-ac33-b55a9a4d1fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time/fps</th>\n",
       "      <th>time/total_timesteps</th>\n",
       "      <th>time/iterations</th>\n",
       "      <th>time/time_elapsed</th>\n",
       "      <th>train/value_loss</th>\n",
       "      <th>train/learning_rate</th>\n",
       "      <th>train/clip_fraction</th>\n",
       "      <th>train/n_updates</th>\n",
       "      <th>train/clip_range</th>\n",
       "      <th>train/explained_variance</th>\n",
       "      <th>train/entropy_loss</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>train/policy_gradient_loss</th>\n",
       "      <th>train/approx_kl</th>\n",
       "      <th>rollout/ep_rew_mean</th>\n",
       "      <th>rollout/ep_len_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>1024</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>2473.499306</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.697110e-04</td>\n",
       "      <td>-1.945902</td>\n",
       "      <td>760.137024</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>1.059147e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>1536</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>287.685600</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.889271e-04</td>\n",
       "      <td>-1.945902</td>\n",
       "      <td>200.912506</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>1.050415e-06</td>\n",
       "      <td>1701.000000</td>\n",
       "      <td>1265.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>73</td>\n",
       "      <td>7064.046777</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.965233e-04</td>\n",
       "      <td>-1.945901</td>\n",
       "      <td>3429.449951</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>6.536720e-07</td>\n",
       "      <td>1600.500000</td>\n",
       "      <td>854.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>2560</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>3292.192752</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.963445e-04</td>\n",
       "      <td>-1.945902</td>\n",
       "      <td>1477.534302</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>9.706710e-07</td>\n",
       "      <td>1600.500000</td>\n",
       "      <td>854.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>3072</td>\n",
       "      <td>6</td>\n",
       "      <td>112</td>\n",
       "      <td>999.817033</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-2.502203e-04</td>\n",
       "      <td>-1.945902</td>\n",
       "      <td>509.015228</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>2.779998e-06</td>\n",
       "      <td>1646.666667</td>\n",
       "      <td>889.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27</td>\n",
       "      <td>3584</td>\n",
       "      <td>7</td>\n",
       "      <td>132</td>\n",
       "      <td>3170.068651</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-7.152557e-07</td>\n",
       "      <td>-1.945899</td>\n",
       "      <td>1728.846558</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>5.359761e-07</td>\n",
       "      <td>1625.500000</td>\n",
       "      <td>876.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>4096</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>3296.370892</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.819300e-04</td>\n",
       "      <td>-1.945894</td>\n",
       "      <td>2055.074951</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>7.844064e-07</td>\n",
       "      <td>1625.500000</td>\n",
       "      <td>876.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time/fps  time/total_timesteps  time/iterations  time/time_elapsed  \\\n",
       "0        33                   512                1                 15   \n",
       "1        28                  1024                2                 35   \n",
       "2        28                  1536                3                 54   \n",
       "3        27                  2048                4                 73   \n",
       "4        27                  2560                5                 92   \n",
       "5        27                  3072                6                112   \n",
       "6        27                  3584                7                132   \n",
       "7        27                  4096                8                150   \n",
       "\n",
       "   train/value_loss  train/learning_rate  train/clip_fraction  \\\n",
       "0          0.000000             0.000000                  0.0   \n",
       "1       2473.499306             0.000001                  0.0   \n",
       "2        287.685600             0.000001                  0.0   \n",
       "3       7064.046777             0.000001                  0.0   \n",
       "4       3292.192752             0.000001                  0.0   \n",
       "5        999.817033             0.000001                  0.0   \n",
       "6       3170.068651             0.000001                  0.0   \n",
       "7       3296.370892             0.000001                  0.0   \n",
       "\n",
       "   train/n_updates  train/clip_range  train/explained_variance  \\\n",
       "0              0.0               0.0              0.000000e+00   \n",
       "1             10.0               0.2              2.697110e-04   \n",
       "2             20.0               0.2              7.889271e-04   \n",
       "3             30.0               0.2              5.965233e-04   \n",
       "4             40.0               0.2              5.963445e-04   \n",
       "5             50.0               0.2             -2.502203e-04   \n",
       "6             60.0               0.2             -7.152557e-07   \n",
       "7             70.0               0.2              2.819300e-04   \n",
       "\n",
       "   train/entropy_loss   train/loss  train/policy_gradient_loss  \\\n",
       "0            0.000000     0.000000                    0.000000   \n",
       "1           -1.945902   760.137024                   -0.000094   \n",
       "2           -1.945902   200.912506                   -0.000070   \n",
       "3           -1.945901  3429.449951                   -0.000012   \n",
       "4           -1.945902  1477.534302                    0.000038   \n",
       "5           -1.945902   509.015228                   -0.000125   \n",
       "6           -1.945899  1728.846558                   -0.000013   \n",
       "7           -1.945894  2055.074951                   -0.000047   \n",
       "\n",
       "   train/approx_kl  rollout/ep_rew_mean  rollout/ep_len_mean  \n",
       "0     0.000000e+00             0.000000             0.000000  \n",
       "1     1.059147e-06             0.000000             0.000000  \n",
       "2     1.050415e-06          1701.000000          1265.000000  \n",
       "3     6.536720e-07          1600.500000           854.500000  \n",
       "4     9.706710e-07          1600.500000           854.500000  \n",
       "5     2.779998e-06          1646.666667           889.666667  \n",
       "6     5.359761e-07          1625.500000           876.000000  \n",
       "7     7.844064e-07          1625.500000           876.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f70d5620-5013-454b-b4f2-616f1226061a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ypoints = np.array(df['rollout/ep_rew_mean'].to_numpy())\n",
    "plt.plot(ypoints, color = 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b663d6-613f-4afe-a723-324dc9d4cbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
