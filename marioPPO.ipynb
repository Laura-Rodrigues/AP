{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d738b0-7c19-4a8a-872d-741ce3b32ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edu/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/edu/anaconda3/envs/tp_ap/lib/python3.9/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode=\"human\")\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fddc8e94-c13f-4ca5-8600-2c51262682a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, ResizeObservation\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86352ad-4555-4891-9981-28947639fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(PPO, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.linear = nn.Linear(32 * 6 * 6, 512)\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "        self.actor_linear = nn.Linear(512, num_actions)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, nn.init.calculate_gain('relu'))\n",
    "                # nn.init.xavier_uniform_(module.weight)\n",
    "                # nn.init.kaiming_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        #x = self.linear(x.view(x.size(0), -1))\n",
    "        x = self.linear(x.reshape(x.size(0), -1))\n",
    "\n",
    "        return self.actor_linear(x), self.critic_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd46d36-db28-417b-9e63-c32e15611ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def eval(global_model):\n",
    "    torch.manual_seed(123)\n",
    "    \n",
    "    local_model = PPO(1, env.action_space.n)\n",
    "\n",
    "    local_model.eval()\n",
    "    #state = torch.from_numpy(env.reset())\n",
    "    state, _ = env.reset()\n",
    "    state = torch.from_numpy(np.array(state)).float().permute(0, 3, 1, 2)\n",
    "   \n",
    "    done = True\n",
    "    curr_step = 0\n",
    "    actions = deque(maxlen=200)\n",
    "    while True:\n",
    "        curr_step += 1\n",
    "        if done:\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "        logits, value = local_model(state)\n",
    "        policy = F.softmax(logits, dim=1)\n",
    "        action = torch.argmax(policy).item()\n",
    "        state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Uncomment following lines if you want to save model whenever level is completed\n",
    "        # if info[\"flag_get\"]:\n",
    "        #     print(\"Finished\")\n",
    "        #     torch.save(local_model.state_dict(),\n",
    "        #                \"{}/ppo_super_mario_bros_{}_{}_{}\".format(opt.saved_path, opt.world, opt.stage, curr_step))\n",
    "\n",
    "        env.render()\n",
    "        actions.append(action)\n",
    "        if curr_step > 5e6 or actions.count(actions[0]) == actions.maxlen:\n",
    "            done = True\n",
    "        if done:\n",
    "            curr_step = 0\n",
    "            actions.clear()\n",
    "            state, _ = env.reset()\n",
    "        state, _ = env.reset()\n",
    "        state = torch.from_numpy(np.array(state)).float().permute(0, 3, 1, 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9758948c-9556-4d70-a1b6-e3b03d60654e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got LazyFrames)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m                 optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Total loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(curr_episode, total_loss))\n\u001b[0;32m--> 136\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     curr_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(curr_state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(curr_state, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m#curr_state = torch.Tensor(curr_state).permute(0, 3, 1, 2).float()\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     curr_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m curr_state])\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#curr_state = torch.from_numpy(np.array(curr_state)).permute(0, 3, 1, 2)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(curr_state, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m     curr_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(curr_state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(curr_state, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m#curr_state = torch.Tensor(curr_state).permute(0, 3, 1, 2).float()\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     curr_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m curr_state])\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#curr_state = torch.from_numpy(np.array(curr_state)).permute(0, 3, 1, 2)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(curr_state, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got LazyFrames)"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from gym.wrappers.frame_stack import LazyFrames\n",
    "\n",
    "\n",
    "gamma = 0.9\n",
    "num_local_steps = 512\n",
    "tau = 1.0\n",
    "beta = 0.01\n",
    "num_epochs = 10\n",
    "num_processes=1\n",
    "batch_size = 32\n",
    "lr = 0.0001\n",
    "epsilon = 0.2\n",
    "\n",
    "def train():\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    #if os.path.isdir(opt.log_path):\n",
    "    #    shutil.rmtree(opt.log_path)\n",
    "    #os.makedirs(opt.log_path)\n",
    "    #if not os.path.isdir(opt.saved_path):\n",
    "    #    os.makedirs(opt.saved_path)\n",
    "    model = PPO(1, env.action_space.n)\n",
    "    model.share_memory()\n",
    "    #process = mp.Process(target=eval, args=(opt, model, envs.num_states, envs.num_actions))\n",
    "    #process.start()\n",
    "    #eval(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    #[agent_conn.send((\"reset\", None)) for agent_conn in envs.agent_conns]\n",
    "    #curr_states = [agent_conn.recv() for agent_conn in envs.agent_conns]\n",
    "    #curr_states = torch.from_numpy(np.concatenate(curr_states, 0))\n",
    "\n",
    "    curr_episode = 0\n",
    "    while True:\n",
    "        # if curr_episode % opt.save_interval == 0 and curr_episode > 0:\n",
    "        #     torch.save(model.state_dict(),\n",
    "        #                \"{}/ppo_super_mario_bros_{}_{}\".format(opt.saved_path, opt.world, opt.stage))\n",
    "        #     torch.save(model.state_dict(),\n",
    "        #                \"{}/ppo_super_mario_bros_{}_{}_{}\".format(opt.saved_path, opt.world, opt.stage, curr_episode))\n",
    "        curr_episode += 1\n",
    "        \n",
    "        curr_state, _ = env.reset()\n",
    "        curr_state = torch.from_numpy(np.array(curr_state)).float().permute(0, 3, 1, 2)\n",
    "        \n",
    "        old_log_policies = []\n",
    "        actions = []\n",
    "        values = []\n",
    "        states = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        for _ in range(num_local_steps):\n",
    "            states.append(curr_state)\n",
    "            \n",
    "            if isinstance(curr_state, LazyFrames):\n",
    "                curr_state = torch.from_numpy(np.array(curr_state._frames)).float().permute(0, 3, 1, 2)\n",
    "\n",
    "            \n",
    "            logits, value = model(curr_state.float())\n",
    "            values.append(value.squeeze())\n",
    "            policy = F.softmax(logits, dim=1)\n",
    "            old_m = Categorical(policy)\n",
    "            action = old_m.sample()\n",
    "            actions.append(action)\n",
    "            old_log_policy = old_m.log_prob(action)\n",
    "            old_log_policies.append(old_log_policy)\n",
    "          \n",
    "            if action.nelement() > 1:  # if action has more than one element\n",
    "                action = action[0] \n",
    "\n",
    "            curr_state, reward, done, trunc, info = env.step(action.item())\n",
    "            \n",
    "            if done:\n",
    "                curr_state = env.reset() \n",
    "            \n",
    "            if isinstance(curr_state, list):\n",
    "                curr_state = np.stack(curr_state, axis=0)\n",
    "                \n",
    "            if isinstance(curr_state, tuple):\n",
    "                #curr_state = torch.Tensor(curr_state).permute(0, 3, 1, 2).float()\n",
    "                curr_state = torch.stack([torch.from_numpy(np.array(x._frames)) if isinstance(x, LazyFrames) else torch.from_numpy(x) for x in curr_state]).permute(0, 3, 1, 2).float()\n",
    "\n",
    "            #curr_state = torch.from_numpy(np.array(curr_state)).permute(0, 3, 1, 2)\n",
    "            if isinstance(curr_state, np.ndarray):\n",
    "                curr_state = torch.from_numpy(curr_state).permute(0, 3, 1, 2).float()\n",
    "            \n",
    "           \n",
    "            reward = torch.FloatTensor([reward])\n",
    "            done = torch.FloatTensor([done])\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            curr_states = states\n",
    "\n",
    "        _, next_value, = model(curr_states)\n",
    "        next_value = next_value.squeeze()\n",
    "        old_log_policies = torch.cat(old_log_policies).detach()\n",
    "        actions = torch.cat(actions)\n",
    "        values = torch.cat(values).detach()\n",
    "        states = torch.cat(states)\n",
    "        gae = 0\n",
    "        R = []\n",
    "        for value, reward, done in list(zip(values, rewards, dones))[::-1]:\n",
    "            gae = gae * gamma * tau\n",
    "            gae = gae + reward + gamma * next_value.detach() * (1 - done) - value.detach()\n",
    "            next_value = value\n",
    "            R.append(gae + value)\n",
    "        R = R[::-1]\n",
    "        R = torch.cat(R).detach()\n",
    "        advantages = R - values\n",
    "        for i in range(num_epochs):\n",
    "            indice = torch.randperm(num_local_steps * num_processes)\n",
    "            for j in range(batch_size):\n",
    "                batch_indices = indice[\n",
    "                                int(j * (num_local_steps * num_processes / batch_size)): int((j + 1) * (\n",
    "                                        num_local_steps * num_processes / batch_size))]\n",
    "                logits, value = model(states[batch_indices])\n",
    "                new_policy = F.softmax(logits, dim=1)\n",
    "                new_m = Categorical(new_policy)\n",
    "                new_log_policy = new_m.log_prob(actions[batch_indices])\n",
    "                ratio = torch.exp(new_log_policy - old_log_policies[batch_indices])\n",
    "                actor_loss = -torch.mean(torch.min(ratio * advantages[batch_indices],\n",
    "                                                   torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon) *\n",
    "                                                   advantages[\n",
    "                                                       batch_indices]))\n",
    "                # critic_loss = torch.mean((R[batch_indices] - value) ** 2) / 2\n",
    "                critic_loss = F.smooth_l1_loss(R[batch_indices], value.squeeze())\n",
    "                entropy_loss = torch.mean(new_m.entropy())\n",
    "                total_loss = actor_loss + critic_loss - beta * entropy_loss\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "        print(\"Episode: {}. Total loss: {}\".format(curr_episode, total_loss))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c7ea6-f09d-4347-9d3e-8b3473b51b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a374e-7beb-4093-999b-6d75c4a14b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
